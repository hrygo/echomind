# EchoMind æ—¥å¿—æ¡†æ¶æ€§èƒ½ä¼˜åŒ–æŒ‡å—

## ğŸ“‹ ç›®å½•

1. [æ€§èƒ½åŸºå‡†](#æ€§èƒ½åŸºå‡†)
2. [æ‰¹é‡å¤„ç†ä¼˜åŒ–](#æ‰¹é‡å¤„ç†ä¼˜åŒ–)
3. [å†…å­˜ä¼˜åŒ–](#å†…å­˜ä¼˜åŒ–)
4. [ç½‘ç»œä¼ è¾“ä¼˜åŒ–](#ç½‘ç»œä¼ è¾“ä¼˜åŒ–)
5. [å­˜å‚¨ä¼˜åŒ–](#å­˜å‚¨ä¼˜åŒ–)
6. [å¹¶å‘ä¼˜åŒ–](#å¹¶å‘ä¼˜åŒ–)
7. [ç›‘æ§å’Œè°ƒä¼˜](#ç›‘æ§å’Œè°ƒä¼˜)

## ğŸ“Š æ€§èƒ½åŸºå‡†

### åŸºå‡†æµ‹è¯•ç»“æœ

```go
// åŸºå‡†æµ‹è¯•é…ç½®
func runBenchmarks() {
    b := testing.B{}

    // åŸºç¡€æ—¥å¿—è®°å½•åŸºå‡†
    b.Run("BasicLogging", benchmarkBasicLogging)
    b.Run("LoggingWithFields", benchmarkLoggingWithFields)
    b.Run("LoggingWithContext", benchmarkLoggingWithContext)

    // æä¾›è€…æ€§èƒ½åŸºå‡†
    b.Run("NoopProvider", benchmarkNoopProvider)
    b.Run("FileProvider", benchmarkFileProvider)
    b.Run("ElasticsearchProvider", benchmarkElasticsearchProvider)
    b.Run("LokiProvider", benchmarkLokiProvider)
}
```

### æ€§èƒ½æŒ‡æ ‡

```go
// æ€§èƒ½æŒ‡æ ‡å¸¸é‡
const (
    TargetLatencyPerLog    = 100 * time.Microsecond // æ¯æ¡æ—¥å¿— 100Î¼s
    TargetThroughputPerSec = 10000               // æ¯ç§’ 10000 æ¡æ—¥å¿—
    TargetMemoryUsage      = 100 * 1024 * 1024      // 100MB å†…å­˜é™åˆ¶
    TargetCPUUsage         = 10                     // 10% CPU ä½¿ç”¨ç‡é™åˆ¶
)

// æ€§èƒ½ç›‘æ§
type PerformanceMetrics struct {
    LatencyPerLog    time.Duration
    ThroughputPerSec float64
    MemoryUsage      int64
    CPUUsage         float64
    ErrorRate        float64
}

func (pm PerformanceMetrics) IsHealthy() bool {
    return pm.LatencyPerLog <= TargetLatencyPerLog &&
           pm.ThroughputPerSec >= TargetThroughputPerSec &&
           pm.MemoryUsage <= TargetMemoryUsage &&
           pm.CPUUsage <= TargetCPUUsage &&
           pm.ErrorRate <= 5.0 // é”™è¯¯ç‡ä¸è¶…è¿‡ 5%
}
```

## ğŸš€ æ‰¹é‡å¤„ç†ä¼˜åŒ–

### 1. æ‰¹é‡å¤„ç†å™¨è®¾è®¡

```go
// é«˜æ•ˆæ‰¹é‡å¤„ç†å™¨
type HighPerformanceBatchProcessor struct {
    provider     Provider
    buffer       []*LogEntry
    bufferLock   sync.Mutex
    bufferCap    int
    flushCh      chan struct{}
    stopCh       chan struct{}
    workers      int
    workQueue    chan *BatchJob
    pool         *sync.Pool
}

// æ‰¹é‡ä½œä¸š
type BatchJob struct {
    Entries []*LogEntry
    Callback func(error)
}

// åˆ›å»ºé«˜æ€§èƒ½æ‰¹é‡å¤„ç†å™¨
func NewHighPerformanceBatchProcessor(provider Provider, config BatchConfig) *HighPerformanceBatchProcessor {
    processor := &HighPerformanceBatchProcessor{
        provider:     provider,
        buffer:       make([]*LogEntry, 0, config.BufferSize),
        bufferCap:    config.BufferSize,
        flushCh:      make(chan struct{}, 1),
        stopCh:       make(chan struct{}),
        workers:      config.Workers,
        workQueue:    make(chan *BatchJob, config.QueueSize),
    }

    // åˆå§‹åŒ–å¯¹è±¡æ± 
    processor.pool = &sync.Pool{
        New: func() interface{} {
            return make([]*LogEntry, 0, 100)
        },
    }

    // å¯åŠ¨å·¥ä½œåç¨‹
    for i := 0; i < processor.workers; i++ {
        go processor.worker(i)
    }

    // å¯åŠ¨åˆ·æ–°åç¨‹
    go processor.flushLoop()

    return processor
}

// å†™å…¥æ—¥å¿—æ¡ç›®
func (bp *HighPerformanceBatchProcessor) Write(ctx context.Context, entry *LogEntry) error {
    // å¿«é€Ÿè·¯å¾„ï¼šå¦‚æœç¼“å†²åŒºæœªæ»¡ï¼Œç›´æ¥æ·»åŠ 
    bp.bufferLock.Lock()
    if len(bp.buffer) < bp.bufferCap {
        bp.buffer = append(bp.buffer, entry)
        bp.bufferLock.Unlock()
        return nil
    }
    bp.bufferLock.Unlock()

    // ç¼“å†²åŒºæ»¡ï¼Œè§¦å‘åˆ·æ–°
    bp.flushCh <- struct{}{}

    // é‡è¯•å†™å…¥
    bp.bufferLock.Lock()
    if len(bp.buffer) < bp.bufferCap {
        bp.buffer = append(bp.buffer, entry)
        bp.bufferLock.Unlock()
        return nil
    }
    bp.bufferLock.Unlock()

    // é™çº§åˆ°åŒæ­¥å†™å…¥
    return bp.provider.Write(ctx, entry)
}
```

### 2. è‡ªé€‚åº”æ‰¹é‡å¤§å°

```go
// è‡ªé€‚åº”æ‰¹é‡å¤§å°æ§åˆ¶å™¨
type AdaptiveBatchController struct {
    minSize    int
    maxSize    int
    currentSize int
    adjustRate  float64
    lastAdjust  time.Time
    metrics     *BatchMetrics
    mu          sync.RWMutex
}

type BatchMetrics struct {
    ProcessCount int64
    ProcessTime  time.Duration
    ErrorCount   int64
    LastFlush    time.Time
}

func NewAdaptiveBatchController(minSize, maxSize int) *AdaptiveBatchController {
    return &AdaptiveBatchController{
        minSize:    minSize,
        maxSize:    maxSize,
        currentSize: minSize,
        adjustRate:  0.1,
        lastAdjust:  time.Now(),
        metrics:     &BatchMetrics{},
    }
}

// è‡ªé€‚åº”è°ƒæ•´æ‰¹é‡å¤§å°
func (abc *AdaptiveBatchController) AdjustBatchSize() {
    abc.mu.Lock()
    defer abc.mu.Unlock()

    timeSinceLastAdjust := time.Since(abc.lastAdjust)
    if timeSinceLastAdjust < 10*time.Second {
        return
    }

    // è®¡ç®—å¤„ç†é€Ÿåº¦ï¼ˆæ¡/ç§’ï¼‰
    var processSpeed float64
    if abc.metrics.ProcessCount > 0 {
        processSpeed = float64(abc.metrics.ProcessCount) / abc.metrics.ProcessTime.Seconds()
    }

    // è®¡ç®—é”™è¯¯ç‡
    var errorRate float64
    if abc.metrics.ProcessCount > 0 {
        errorRate = float64(abc.metrics.ErrorCount) / float64(abc.metrics.ProcessCount)
    }

    // æ ¹æ®æŒ‡æ ‡è°ƒæ•´æ‰¹é‡å¤§å°
    newSize := abc.currentSize
    switch {
    case processSpeed > 1000 && errorRate < 0.01: // é«˜æ€§èƒ½ï¼Œä½é”™è¯¯ç‡
        newSize = int(float64(abc.currentSize) * (1 + abc.adjustRate))
    case processSpeed < 100 || errorRate > 0.05: // ä½æ€§èƒ½æˆ–é«˜é”™è¯¯ç‡
        newSize = int(float64(abc.currentSize) * (1 - abc.adjustRate))
    }

    // é™åˆ¶åœ¨ min/max èŒƒå›´å†…
    newSize = max(abc.minSize, min(abc.maxSize, newSize))

    if newSize != abc.currentSize {
        abc.currentSize = newSize
        abc.lastAdjust = time.Now()

        logger.Debug("è°ƒæ•´æ‰¹é‡å¤§å°",
            logger.Int("old_size", abc.currentSize),
            logger.Int("new_size", newSize),
            logger.Float64("process_speed", processSpeed),
            logger.Float64("error_rate", errorRate))
    }

    // é‡ç½®æŒ‡æ ‡
    abc.metrics = &BatchMetrics{}
}
```

### 3. é¢„åˆ†é…å’Œå¯¹è±¡æ± 

```go
// é¢„åˆ†é…çš„å­—æ®µæ± 
type FieldPool struct {
    stringPool   sync.Pool
    intPool     sync.Pool
    boolPool    sync.Pool
    timePool    sync.Pool
    float64Pool sync.Pool
    anyPool     sync.Pool
}

func NewFieldPool() *FieldPool {
    return &FieldPool{
        stringPool: sync.Pool{
            New: func() interface{} {
                return make([]string, 0, 20)
            },
        },
        intPool: sync.Pool{
            New: func() interface{} {
                return make([]int, 0, 20)
            },
        },
        boolPool: sync.Pool{
            New: func() interface{} {
                return make([]bool, 0, 20)
            },
        },
        timePool: sync.Pool{
            New: func() interface{} {
                return make([]time.Time, 0, 20)
            },
        },
        float64Pool: sync.Pool{
            New: func() interface{} {
                return make([]float64, 0, 20)
            },
        },
        anyPool: sync.Pool{
            New: func() interface{} {
                return make([]interface{}, 0, 20)
            },
        },
    }
}

// ä½¿ç”¨å¯¹è±¡æ± åˆ›å»ºå­—æ®µ
func (fp *FieldPool) String(key, value string) logger.Field {
    return logger.Field{Key: key, Value: value}
}

// æ‰¹é‡å­—æ®µåˆ›å»º
func (fp *FieldPool) CreateBatch(count int) []logger.Field {
    fields := make([]logger.Field, 0, count)
    stringFields := fp.stringPool.Get().([]string)
    defer fp.stringPool.Put(stringFields)

    // é‡ç”¨ string æ•°ç»„
    *stringFields = (*stringFields)[:0]

    for i := 0; i < count; i++ {
        fields = append(fields, logger.Field{Key: fmt.Sprintf("field_%d", i), Value: ""})
    }

    return fields
}
```

## ğŸ’¾ å†…å­˜ä¼˜åŒ–

### 1. å†…å­˜æ± åŒ–

```go
// å†…å­˜æ± ç®¡ç†å™¨
type MemoryPoolManager struct {
    entryPools   map[int]*sync.Pool // æŒ‰å¤§å°åˆ†ç»„çš„ LogEntry æ± 
    bufferPools map[int]*sync.Pool // æŒ‰å¤§å°åˆ†ç»„çš„ç¼“å†²åŒºæ± 
    mu          sync.RWMutex
}

func NewMemoryPoolManager() *MemoryPoolManager {
    mpm := &MemoryPoolManager{
        entryPools:   make(map[int]*sync.Pool),
        bufferPools: make(map[int]*sync.Pool),
    }

    // åˆå§‹åŒ–ä¸åŒå¤§å°çš„æ± 
    sizes := []int{64, 128, 256, 512, 1024, 2048, 4096}
    for _, size := range sizes {
        mpm.entryPools[size] = &sync.Pool{
            New: func() interface{} {
                return make([]*LogEntry, 0, size)
            },
        }
        mpm.bufferPools[size] = &sync.Pool{
            New: func() interface{} {
                return make([]byte, 0, size)
            },
        }
    }

    return mpm
}

// è·å– LogEntry åˆ‡ç‰‡
func (mpm *MemoryPoolManager) GetEntrySlice(size int) []*LogEntry {
    // æ‰¾åˆ°åˆé€‚å¤§å°çš„æ± 
    poolSize := roundUpPowerOfTwo(size)
    if poolSize > 4096 {
        poolSize = 4096
    }

    mpm.mu.RLock()
    pool := mpm.entryPools[poolSize]
    mpm.RUnlock()

    if pool != nil {
        return pool.Get().([]*LogEntry)
    }

    // é™çº§åˆ°ç›´æ¥åˆ†é…
    return make([]*LogEntry, 0, size)
}

// å½’è¿˜ LogEntry åˆ‡ç‰‡
func (mpm *MemoryPoolManager) PutEntrySlice(entries []*LogEntry) {
    if len(entries) == 0 {
        return
    }

    size := cap(entries)
    poolSize := roundUpPowerOfTwo(size)
    if poolSize > 4096 {
        return
    }

    mpm.mu.RLock()
    pool := mpm.entryPools[poolSize]
    mpm.RUnlock()

    if pool != nil {
        // æ¸…ç©ºåˆ‡ç‰‡ä½†ä¿æŒå®¹é‡
        entries = entries[:0]
        pool.Put(entries)
    }
}

func roundUpPowerOfTwo(n int) int {
    if n <= 0 {
        return 1
    }
    n--
    n |= n >> 1
    n |= n >> 2
    n |= n >> 4
    n |= n >> 8
    n |= n >> 16
    n++
    return n
}
```

### 2. å»¶è¿Ÿåºåˆ—åŒ–

```go
// å»¶è¿Ÿåºåˆ—åŒ–å™¨
type LazySerializer struct {
    data    interface{}
    cache   []byte
    cached  bool
    marshal func(interface{}) ([]byte, error)
}

func NewLazySerializer(marshal func(interface{}) ([]byte, error)) *LazySerializer {
    return &LazySerializer{
        marshal: marshal,
    }
}

func (ls *LazySerializer) SetData(data interface{}) {
    ls.data = data
    ls.cached = false
}

func (ls *LazySerializer) Bytes() ([]byte, error) {
    if !ls.cached {
        serialized, err := ls.marshal(ls.data)
        if err != nil {
            return nil, err
        }
        ls.cache = serialized
        ls.cached = true
    }
    return ls.cache, nil
}

// åœ¨ LogEntry ä¸­ä½¿ç”¨å»¶è¿Ÿåºåˆ—åŒ–
type LogEntry struct {
    Timestamp time.Time
    Level     Level
    Message   string
    Fields    *LazyFields
    Context   ContextInfo
    Source    SourceInfo
}

type LazyFields struct {
    data    map[string]interface{}
    cache   []byte
    cached  bool
    marshal func(map[string]interface{}) ([]byte, error)
}

func (lf *LazyFields) Set(key string, value interface{}) {
    lf.data[key] = value
    lf.cached = false
}

func (lf *LazyFields) Bytes() ([]byte, error) {
    if !lf.cached {
        serialized, err := lf.marshal(lf.data)
        if err != nil {
            return nil, err
        }
        lf.cache = serialized
        lf.cached = true
    }
    return lf.cache, nil
}
```

### 3. å­—ç¬¦ä¸²æ„å»ºä¼˜åŒ–

```go
// é«˜æ•ˆå­—ç¬¦ä¸²æ„å»ºå™¨
type StringBuilder struct {
    builder strings.Builder
    pool    *sync.Pool
}

func NewStringBuilder() *StringBuilder {
    return &StringBuilder{
        pool: &sync.Pool{
            New: func() interface{} {
                return &StringBuilder{
                    builder: strings.Builder{},
                }
            },
        },
    }
}

// è·å–å­—ç¬¦ä¸²æ„å»ºå™¨
func (sb *StringBuilder) Get() *strings.Builder {
    return sb.pool.Get().(*strings.Builder)
}

// å½’è¿˜å­—ç¬¦ä¸²æ„å»ºå™¨
func (sb *StringBuilder) Put(builder *strings.Builder) {
    builder.Reset()
    sb.pool.Put(builder)
}

// æ ¼å¼åŒ–æ—¥å¿—æ¶ˆæ¯
func (sb *StringBuilder) FormatMessage(template string, args ...interface{}) string {
    builder := sb.Get()
    defer sb.Put(builder)

    builder.Grow(len(template) + len(args)*10) // é¢„ä¼°å®¹é‡

    if len(args) == 0 {
        builder.WriteString(template)
    } else {
        fmt.Fprintf(builder, template, args...)
    }

    return builder.String()
}

// æ„å»º JSON
func (sb *StringBuilder) BuildJSON(fields map[string]interface{}) string {
    builder := sb.Get()
    defer sb.Put(builder)

    builder.Grow(512) // é¢„ä¼° JSON å¤§å°

    builder.WriteByte('{')
    first := true
    for key, value := range fields {
        if !first {
            builder.WriteByte(',')
        }
        first = false

        // å†™å…¥ key
        builder.WriteByte('"')
        builder.WriteString(key)
        builder.WriteString("\":")

        // å†™å…¥ valueï¼ˆç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä½¿ç”¨ json.Marshalï¼‰
        builder.WriteString(fmt.Sprintf("%v", value))
    }
    builder.WriteByte('}')

    return builder.String()
}
```

## ğŸŒ ç½‘ç»œä¼ è¾“ä¼˜åŒ–

### 1. è¿æ¥æ± ç®¡ç†

```go
// HTTP è¿æ¥æ± 
type HTTPConnectionPool struct {
    client     *http.Client
    transports []*http.Transport
    mu         sync.Mutex
    maxIdle    int
    current    int
}

func NewHTTPConnectionPool(maxIdle int) *HTTPConnectionPool {
    pool := &HTTPConnectionPool{
        maxIdle: maxIdle,
    }

    // é¢„åˆ›å»ºä¼ è¾“å±‚
    for i := 0; i < maxIdle; i++ {
        transport := &http.Transport{
            MaxIdleConns:        100,
            MaxIdleConnsPerHost: 10,
            IdleConnTimeout:     90 * time.Second,
            DisableCompression:  false,
        }
        pool.transports = append(pool.transports, transport)
    }

    pool.client = &http.Client{
        Transport: pool.getNextTransport(),
    }

    return pool
}

func (pool *HTTPConnectionPool) getNextTransport() *http.Transport {
    pool.mu.Lock()
    defer pool.mu.Unlock()

    if len(pool.transports) > 0 {
        transport := pool.transports[len(pool.transports)-1]
        pool.transports = pool.transports[:len(pool.transports)-1]
        return transport
    }

    // å¦‚æœæ± ç©ºï¼Œåˆ›å»ºæ–°çš„ä¼ è¾“å±‚
    return &http.Transport{
        MaxIdleConns:        100,
        MaxIdleConnsPerHost: 10,
        IdleConnTimeout:     90 * time.Second,
    }
}

func (pool *HTTPConnectionPool) ReturnTransport(transport *http.Transport) {
    pool.mu.Lock()
    defer pool.mu.Unlock()

    if len(pool.transports) < pool.maxIdle {
        // é‡ç½®ä¼ è¾“å±‚çŠ¶æ€
        transport.CloseIdleConnections()
        pool.transports = append(pool.transports, transport)
    } else {
        // æ± æ»¡äº†ï¼Œç›´æ¥å…³é—­
        transport.Close()
    }
}

func (pool *HTTPConnectionPool) Close() error {
    pool.mu.Lock()
    defer pool.mu.Unlock()

    for _, transport := range pool.transports {
        transport.CloseIdleConnections()
    }

    pool.client.CloseIdleConnections()
    return nil
}
```

### 2. è¯·æ±‚æ‰¹é‡åŒ–

```go
// è¯·æ±‚æ‰¹é‡å¤„ç†å™¨
type RequestBatchProcessor struct {
    pool       *HTTPConnectionPool
    batchSize  int
    timeout    time.Duration
    mu         sync.Mutex
    pending    []PendingRequest
}

type PendingRequest struct {
    Request  *http.Request
    Response chan *http.Response
    Error    chan error
}

func NewRequestBatchProcessor(pool *HTTPConnectionPool, batchSize int, timeout time.Duration) *RequestBatchProcessor {
    return &RequestBatchProcessor{
        pool:      pool,
        batchSize: batchSize,
        timeout:   timeout,
        pending:   make([]PendingRequest, 0),
    }
}

// å‘é€è¯·æ±‚
func (rbp *RequestBatchProcessor) Send(req *http.Request) (*http.Response, error) {
    respCh := make(chan *http.Response, 1)
    errCh := make(chan error, 1)

    pending := PendingRequest{
        Request:  req,
        Response: respCh,
        Error:    errCh,
    }

    rbp.mu.Lock()
    rbp.pending = append(rbp.pending, pending)
    shouldFlush := len(rbp.pending) >= rbp.batchSize
    rbp.mu.Unlock()

    if shouldFlush {
        go rbp.flush()
    }

    select {
    case resp := <-respCh:
        return resp, nil
    case err := <-errCh:
        return nil, err
    case <-time.After(rbp.timeout):
        return nil, fmt.Errorf("request timeout after %v", rbp.timeout)
    }
}

// æ‰¹é‡åˆ·æ–°
func (rbp *RequestBatchProcessor) flush() error {
    rbp.mu.Lock()
    if len(rbp.pending) == 0 {
        rbp.mu.Unlock()
        return nil
    }

    // åˆ›å»ºæ‰¹æ¬¡å‰¯æœ¬
    batch := make([]PendingRequest, len(rbp.pending))
    copy(batch, rbp.pending)
    rbp.pending = rbp.pending[:0]
    rbp.mu.Unlock()

    // å¹¶å‘å¤„ç†æ‰¹æ¬¡
    var wg sync.WaitGroup
    errCh := make(chan error, len(batch))

    for i, pending := range batch {
        wg.Add(1)
        go func(idx int, p PendingRequest) {
            defer wg.Done()
            errCh[idx] = rbp.processRequest(p)
        }(i, pending)
    }

    wg.Wait()

    // æ£€æŸ¥é”™è¯¯
    for _, err := range errCh {
        if err != nil {
            return err
        }
    }

    return nil
}

func (rbp *RequestBatchProcessor) processRequest(pending PendingRequest) error {
    transport := rbp.pool.getNextTransport()
    client := &http.Client{
        Transport: transport,
        Timeout:  5 * time.Second,
    }

    defer rbp.pool.ReturnTransport(transport)

    resp, err := client.Do(pending.Request)
    if err != nil {
        pending.Error <- err
        return err
    }

    pending.Response <- resp
    return nil
}
```

### 3. å‹ç¼©ä¼ è¾“

```go
// å‹ç¼©ä¼ è¾“å®¢æˆ·ç«¯
type CompressingClient struct {
    client     *http.Client
    compressor  *Compressor
}

type Compressor struct {
    level int
}

func NewCompressingClient() *CompressingClient {
    pool := NewHTTPConnectionPool(10)

    return &CompressingClient{
        client: pool.client,
        compressor: &Compressor{
            level: gzip.BestCompression,
        },
    }
}

func (cc *CompressingClient) SendJSON(url string, data interface{}) (*http.Response, error) {
    // åºåˆ—åŒ– JSON
    jsonData, err := json.Marshal(data)
    if err != nil {
        return nil, err
    }

    // å‹ç¼©æ•°æ®
    compressedData := cc.compressor.Compress(jsonData)

    // åˆ›å»ºè¯·æ±‚
    req, err := http.NewRequest("POST", url, bytes.NewReader(compressedData))
    if err != nil {
        return nil, err
    }

    req.Header.Set("Content-Type", "application/json")
    req.Header.Set("Content-Encoding", "gzip")
    req.Header.Set("Content-Length", fmt.Sprintf("%d", len(compressedData)))

    // å‘é€è¯·æ±‚
    return cc.client.Do(req)
}

// Gzip å‹ç¼©å™¨
func (c *Compressor) Compress(data []byte) []byte {
    var buf bytes.Buffer
    gz := gzip.NewWriter(&buf)
    gz.Level = c.level

    if _, err := gz.Write(data); err != nil {
        gz.Close()
        return data
    }
    gz.Close()

    return buf.Bytes()
}
```

## ğŸ’¾ å­˜å‚¨ä¼˜åŒ–

### 1. æ—¥å¿—è½®è½¬ä¼˜åŒ–

```go
// æ™ºèƒ½æ—¥å¿—è½®è½¬å™¨
type SmartLogRotator struct {
    writer      io.Writer
    maxSize     int64
    maxAge      time.Duration
    maxBackups  int
    currentSize  int64
    currentAge  time.Time
    mu           sync.Mutex
    stats       *RotationStats
}

type RotationStats struct {
    Rotations   int64
    TotalBytes   int64
    CompressedBytes int64
}

func NewSmartLogRotator(writer io.Writer, maxSize int64, maxAge time.Duration, maxBackups int) *SmartLogRotator {
    return &SmartLogRotator{
        writer:     writer,
        maxSize:     maxSize,
        maxAge:      maxAge,
        maxBackups:  maxBackups,
        currentSize: 0,
        currentAge:  time.Now(),
        stats:       &RotationStats{},
    }
}

// å†™å…¥æ•°æ®
func (slr *SmartLogRotator) Write(data []byte) (int, error) {
    slr.mu.Lock()
    defer slr.mu.Unlock()

    // æ£€æŸ¥æ˜¯å¦éœ€è¦è½®è½¬
    shouldRotate := slr.shouldRotate(len(data))
    if shouldRotate {
        if err := slr.rotate(); err != nil {
            return 0, err
        }
    }

    n, err := slr.writer.Write(data)
    if err != nil {
        return n, err
    }

    slr.currentSize += int64(n)
    return n, nil
}

func (slr *SmartLogRotator) shouldRotate(dataSize int) bool {
    // æ£€æŸ¥å¤§å°é™åˆ¶
    if slr.currentSize+int64(dataSize) >= slr.maxSize {
        return true
    }

    // æ£€æŸ¥æ—¶é—´é™åˆ¶
    if time.Since(slr.currentAge) >= slr.maxAge {
        return true
    }

    return false
}

func (slr *SmartLogRotator) rotate() error {
    // æ›´æ–°ç»Ÿè®¡
    slr.stats.Rotations++
    slr.stats.TotalBytes += slr.currentSize

    // åˆ›å»ºå¤‡ä»½æ–‡ä»¶å
    timestamp := time.Now().Format("2006-01-02-15-04-05")
    backupPath := fmt.Sprintf("/var/log/app.%s.%d.log", timestamp, slr.stats.Rotations)

    // å‹ç¼©å¤‡ä»½æ–‡ä»¶
    if err := slr.compressAndBackup(backupPath); err != nil {
        logger.Error("å‹ç¼©å¤‡ä»½æ–‡ä»¶å¤±è´¥", logger.Error(err))
        // ç»§ç»­æ‰§è¡Œï¼Œä¸ä¸­æ–­æ—¥å¿—è®°å½•
    }

    // é‡ç½®çŠ¶æ€
    slr.currentSize = 0
    slr.currentAge = time.Now()

    return nil
}

func (slr *SmartLogRotator) compressAndBackup(path string) error {
    // è¯»å–å½“å‰æ–‡ä»¶å†…å®¹
    file, err := os.Open(path)
    if err != nil {
        return err
    }
    defer file.Close()

    var buf bytes.Buffer
    gz := gzip.NewWriter(&buf)
    defer gz.Close()

    // å¤åˆ¶å¹¶å‹ç¼©
    if _, err := io.Copy(gz, file); err != nil {
        return err
    }

    // åˆ›å»ºå‹ç¼©æ–‡ä»¶
    compressedPath := path + ".gz"
    compressedFile, err := os.Create(compressedPath)
    if err != nil {
        return err
    }
    defer compressedFile.Close()

    if _, err := compressedFile.Write(buf.Bytes()); err != nil {
        return err
    }

    // æ›´æ–°å‹ç¼©ç»Ÿè®¡
    slr.stats.CompressedBytes += int64(buf.Len())

    // åˆ é™¤åŸå§‹æ–‡ä»¶
    return os.Remove(path)
}
```

### 2. ç´¢å¼•å’Œæœç´¢ä¼˜åŒ–

```go
// ä¼˜åŒ–çš„ç´¢å¼•ç®¡ç†å™¨
type OptimizedIndexManager struct {
    entries    []*LogEntry
    index      map[string][]*LogEntry  // å­—æ®µç´¢å¼•
    timestampIndex []TimeWindow       // æ—¶é—´çª—å£ç´¢å¼•
    mu         sync.RWMutex
    maxEntries int
    windowSize  time.Duration
}

type TimeWindow struct {
    Start    time.Time
    End      time.Time
    Indexes  []*LogEntry
}

func NewOptimizedIndexManager(maxEntries int, windowSize time.Duration) *OptimizedIndexManager {
    return &OptimizedIndexManager{
        entries:       make([]*LogEntry, 0, maxEntries),
        index:         make(map[string][]*LogEntry),
        timestampIndex: make([]TimeWindow, 0),
        maxEntries:    maxEntries,
        windowSize:     windowSize,
    }
}

// æ·»åŠ æ—¥å¿—æ¡ç›®
func (oim *OptimizedIndexManager) Add(entry *LogEntry) error {
    oim.mu.Lock()
    defer oim.mu.Unlock()

    // æ£€æŸ¥å®¹é‡é™åˆ¶
    if len(oim.entries) >= oim.maxEntries {
        oim.cleanup()
    }

    // æ·»åŠ åˆ°ä¸»ç´¢å¼•
    oim.entries = append(oim.entries, entry)

    // æ›´æ–°å­—æ®µç´¢å¼•
    oim.updateFieldIndex(entry)

    // æ›´æ–°æ—¶é—´çª—å£ç´¢å¼•
    oim.updateTimestampIndex(entry)

    return nil
}

// æ¸…ç†æ—§æ¡ç›®
func (oim *OptimizedIndexManager) cleanup() {
    if len(oim.entries) == 0 {
        return
    }

    // åˆ é™¤æœ€æ—§çš„æ¡ç›®
    oldEntry := oim.entries[0]
    oim.entries = oim.entries[1:]

    // æ›´æ–°å­—æ®µç´¢å¼•
    for field, entries := range oim.index {
        // ç§»é™¤å¼•ç”¨
        for i, e := range entries {
            if e == oldEntry {
                oim.index[field] = append(entries[:i], entries[i+1:]...)
                break
            }
        }
    }

    // æ›´æ–°æ—¶é—´çª—å£ç´¢å¼•
    cutoffTime := time.Now().Add(-oim.windowSize)
    for i, window := range oim.timestampIndex {
        if window.End.Before(cutoffTime) {
            oim.timestampIndex = oim.timestampIndex[i+1:]
            break
        }
    }
}

// æŸ¥è¯¢ä¼˜åŒ–
func (oim *OptimizedIndexManager) Query(query Query) ([]*LogEntry, error) {
    oim.mu.RLock()
    defer oim.RUnlock()

    var results []*LogEntry

    // ä½¿ç”¨å­—æ®µç´¢å¼•è¿›è¡Œå¿«é€ŸæŸ¥æ‰¾
    if field, value, ok := query.ExactMatchField(); ok {
        if entries, exists := oim.index[field]; exists {
            for _, entry := range entries {
                if matchesField(entry, field, value) {
                    results = append(results, entry)
                }
            }
            return results, nil
    }

    // å…¨æ–‡æœç´¢
    for _, entry := range oim.entries {
        if matchesQuery(entry, query) {
            results = append(results, entry)
        }
    }

    // æ’åºå’Œé™åˆ¶ç»“æœ
    sort.Slice(results, func(i, j int) bool {
        return results[i].Timestamp.After(results[j].Timestamp)
    })

    if query.Limit > 0 && len(results) > query.Limit {
        results = results[:query.Limit]
    }

    return results, nil
}
```

## ğŸ”§ å¹¶å‘ä¼˜åŒ–

### 1. æ— é”è®¾è®¡

```go
// æ— é”æ—¥å¿—è®°å½•å™¨
type LockFreeLogger struct {
    ringBuffer    []LogEntry
    head         uint64
    tail         uint64
    mask         uint64
    size         int
    writer       chan []LogEntry
    processor    func([]LogEntry)
    stopCh       chan struct{}
    mu           sync.Mutex // ä»…ç”¨äºåœæ­¢
}

func NewLockFreeLogger(size int, processor func([]LogEntry)) *LockFreeLogger {
    // ç¡®ä¿å¤§å°æ˜¯2çš„å¹‚
    size = nextPowerOf2(size)

    logger := &LockFreeLogger{
        ringBuffer: make([]LogEntry, size),
        head:       0,
        tail:       0,
        mask:       uint64(size - 1),
        size:       size,
        writer:     make(chan []LogEntry, 100),
        processor:  processor,
        stopCh:     make(chan struct{}),
    }

    // å¯åŠ¨å†™å…¥åç¨‹
    go logger.writerLoop()
    return logger
}

func (lfl *LockFreeLogger) Write(entry *LogEntry) {
    select {
    case lfl.writer <- []LogEntry{*entry}:
        default:
        // å¦‚æœé˜Ÿåˆ—æ»¡äº†ï¼Œä¸¢å¼ƒæ—¥å¿—ï¼ˆæˆ–æ·»åŠ åˆ°ä¸¢å¼ƒè®¡æ•°ï¼‰
        atomic.AddUint64(&lfl.dropCount, 1)
    }
}

func (lfl *LockFreeLogger) writerLoop() {
    batch := make([]LogEntry, 0, 100)

    for {
        select {
        case entries := <-lfl.writer:
            // æ‰¹é‡æ¥æ”¶æ—¥å¿—æ¡ç›®
            batch = append(batch, entries...)

            // æ‰¹é‡å¤„ç†
            if len(batch) >= 100 {
                lfl.processor(batch)
                batch = batch[:0] // é‡ç”¨åˆ‡ç‰‡
            }

        case <-lfl.stopCh:
            // å¤„ç†å‰©ä½™æ—¥å¿—
            if len(batch) > 0 {
                lfl.processor(batch)
            }
            return
        }
    }
}

func (lfl *LockFreeLogger) Stop() {
    close(lfl.stopCh)
}
```

### 2. åˆ†ç‰‡å¤„ç†

```go
// åˆ†ç‰‡æ—¥å¿—å¤„ç†å™¨
type ShardedLogger struct {
    shards    []*LogShard
    hasher    LogHasher
    shardMask uint64
}

type LogShard struct {
    id       int
    buffer   []*LogEntry
    mutex    sync.Mutex
    processor BatchProcessor
}

func NewShardedLogger(numShards int, processorFactory func(int) BatchProcessor) *ShardedLogger {
    shards := make([]*LogShard, numShards)

    for i := 0; i < numShards; i++ {
        shards[i] = &LogShard{
            id:        i,
            buffer:    make([]*LogEntry, 0, 1000),
            processor: processorFactory(i),
        }
    }

    return &ShardedLogger{
        shards:    shards,
        hasher:    &DefaultLogHasher{},
        shardMask: uint64(numShards - 1),
    }
}

func (sl *ShardedLogger) Write(entry *LogEntry) error {
    shardID := sl.hasher.Hash(entry) & sl.shardMask
    shard := sl.shards[shardID]

    shard.mutex.Lock()
    shard.buffer = append(shard.buffer, entry)
    shouldFlush := len(shard.buffer) >= 1000
    shard.mutex.Unlock()

    if shouldFlush {
        return shard.processor.Process(shard.buffer)
    }

    return nil
}

// æ—¥å¿—å“ˆå¸Œå™¨
type DefaultLogHasher struct{}

func (h *DefaultLogHasher) Hash(entry *LogEntry) uint64 {
    // ä½¿ç”¨æ¶ˆæ¯å†…å®¹çš„å“ˆå¸Œ
    hash := fnv64(entry.Message)
    hash = hash * 31 + uint64(entry.Level)
    hash = hash * 31 + uint64(entry.Timestamp.UnixNano())
    return hash
}

func fnv64(data string) uint64 {
    hash := uint64(2166136261)
    for i := 0; i < len(data); i++ {
        hash ^= uint64(data[i]) * uint64(16777619)
        hash *= 31
    }
    return hash
}
```

### 3. å·¥ä½œçªƒå–è°ƒåº¦

```go
// å·¥ä½œçªƒå–è°ƒåº¦å™¨
type WorkStealingScheduler struct {
    workers      []*Worker
    taskQueue    chan Task
    doneChan     chan struct{}
    workerCount  int
    stealCount   int
}

type Task struct {
    ID     string
    Work   func() error
    Result chan error
    Retry  int
}

type Worker struct {
    id     int
    tasks  chan Task
    done   chan struct{}
    active bool
    mu     sync.Mutex
}

func NewWorkStealingScheduler(workerCount int) *WorkStealingScheduler {
    scheduler := &WorkStealingScheduler{
        workers:     make([]*Worker, workerCount),
        taskQueue:   make(chan Task, 1000),
        doneChan:    make(chan struct{}, workerCount),
        workerCount: workerCount,
    }

    // åˆ›å»ºå·¥ä½œçªƒå–é˜Ÿåˆ—
    queues := make([]chan Task, workerCount)
    for i := 0; i < workerCount; i++ {
        queues[i] = make(chan Task, 100)
        scheduler.workers[i] = &Worker{
            id:     i,
            tasks:  queues[i],
            done:   scheduler.doneChan[i],
        }
    }

    // å¯åŠ¨å·¥ä½œåç¨‹
    for i := 0; i < workerCount; i++ {
        go scheduler.workerLoop(i)
    }

    return scheduler
}

func (wss *WorkStealingScheduler) workerLoop(workerID int) {
    worker := wss.workers[workerID]

    for {
        select {
        case task := <-worker.tasks:
            // å¤„ç†æœ¬åœ°ä»»åŠ¡
            task.Result <- task.Work()
            worker.done <- struct{}{}

        case task := <-wss.stealTask(workerID):
            // å¤„ç†çªƒå–çš„ä»»åŠ¡
            task.Result <- task.Work()
            worker.done <- struct{}{}

        case <-wss.doneChan[workerID]:
            // å·¥ä½œå®Œæˆ
            worker.active = false
            return
        }
    }
}

func (wss *WorkStealingScheduler) stealTask(currentWorkerID int) Task {
    // éšæœºé€‰æ‹©çªƒå–ç›®æ ‡
    targetWorker := (currentWorkerID + 1) % wss.workerCount

    // å°è¯•ä»å…¶ä»–å·¥ä½œé˜Ÿåˆ—çªƒå–
    for i := 1; i < wss.workerCount; i++ {
        targetID := (currentWorkerID + i) % wss.workerCount
        target := wss.workers[targetID]

        target.mutex.Lock()
        select {
        case task := <-target.tasks:
            target.mutex.Unlock()
            return task
        default:
            target.mutex.Unlock()
        }
    }

    // æ— æ³•çªƒå–ï¼Œåˆ›å»ºæ–°ä»»åŠ¡
    return <-wss.taskQueue
}

func (wss *WorkStealingScheduler) SubmitTask(work func() error) error {
    task := Task{
        Work:   work,
        Result: make(chan error, 1),
        Retry:  0,
    }

    select {
    case wss.taskQueue <- task:
        return <-task.Result
    default:
        // é˜Ÿåˆ—æ»¡ï¼Œç›´æ¥æ‰§è¡Œ
        return work()
    }
}
```

## ğŸ“ˆ ç›‘æ§å’Œè°ƒä¼˜

### 1. æ€§èƒ½ç›‘æ§

```go
// æ€§èƒ½ç›‘æ§å™¨
type PerformanceMonitor struct {
    metrics      *PerformanceMetrics
    collectors   []MetricCollector
    interval     time.Duration
    stopCh       chan struct{}
}

type PerformanceMetrics struct {
    Latency    time.Duration
    Throughput float64
    Memory     int64
    CPU        float64
    ErrorRate  float64
    Timestamp  time.Time
}

type MetricCollector interface {
    Collect() *PerformanceMetrics
}

// å¯åŠ¨æ€§èƒ½ç›‘æ§
func StartPerformanceMonitoring() *PerformanceMonitor {
    monitor := &PerformanceMonitor{
        interval:   5 * time.Second,
        stopCh:    make(chan struct{}),
    }

    // æ³¨å†Œæ”¶é›†å™¨
    monitor.collectors = append(monitor.collectors,
        &ThroughputCollector{},
        &LatencyCollector{},
        &MemoryCollector{},
        &ErrorRateCollector{},
    )

    go monitor.monitorLoop()
    return monitor
}

func (pm *PerformanceMonitor) monitorLoop() {
    ticker := time.NewTicker(pm.interval)
    defer ticker.Stop()

    for {
        select {
        case <-ticker.C:
            // æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
            for _, collector := range pm.collectors {
                metrics := collector.Collect()

                // åº”ç”¨æƒé‡è®¡ç®—ç»¼åˆæŒ‡æ ‡
                pm.metrics = calculateWeightedMetrics([]*PerformanceMetrics{metrics})

                // æ£€æŸ¥æ€§èƒ½å¥åº·çŠ¶æ€
                if !pm.metrics.IsHealthy() {
                    pm.handlePerformanceDegradation()
                }

                // è®°å½•æ€§èƒ½æŒ‡æ ‡
                logger.Info("æ€§èƒ½æŒ‡æ ‡",
                    logger.Duration("avg_latency", pm.metrics.Latency),
                    logger.Float64("throughput", pm.metrics.Throughput),
                    logger.Int64("memory_usage", pm.metrics.MemoryUsage),
                    logger.Float64("cpu_usage", pm.metrics.CPUUsage),
                    logger.Float64("error_rate", pm.metrics.ErrorRate))
            }

        case <-pm.stopCh:
            return
        }
    }
}

func calculateWeightedMetrics(metricsList []*PerformanceMetrics) *PerformanceMetrics {
    if len(metricsList) == 0 {
        return &PerformanceMetrics{}
    }

    var totalLatency time.Duration
    var totalThroughput float64
    var totalMemory int64
    var totalCPU float64
    var totalErrorRate float64

    for _, metrics := range metricsList {
        weight := 1.0 / float64(len(metricsList))
        totalLatency += time.Duration(float64(metrics.Latency) * weight)
        totalThroughput += metrics.Throughput * weight
        totalMemory += int64(float64(metrics.Memory) * weight)
        totalCPU += metrics.CPU * weight
        totalErrorRate += metrics.ErrorRate * weight
    }

    return &PerformanceMetrics{
        Latency:    totalLatency,
        Throughput: totalThroughput,
        Memory:     totalMemory,
        CPU:        totalCPU,
        ErrorRate:  totalErrorRate,
        Timestamp:  time.Now(),
    }
}

func (pm *PerformanceMonitor) handlePerformanceDegradation() {
    logger.Error("æ€§èƒ½ä¸‹é™ï¼Œå¯åŠ¨è‡ªåŠ¨ä¼˜åŒ–",
        logger.Duration("avg_latency", pm.metrics.Latency),
        logger.Float64("throughput", pm.metrics.Throughput),
        logger.Float64("error_rate", pm.metrics.ErrorRate))

    // è‡ªåŠ¨ä¼˜åŒ–æªæ–½
    go pm.autoOptimize()
}

func (pm *PerformanceMonitor) autoOptimize() {
    // æ ¹æ®æ€§èƒ½æŒ‡æ ‡è°ƒæ•´é…ç½®
    if pm.metrics.Latency > 2*time.Millisecond {
        // å¢åŠ æ‰¹é‡å¤§å°
        increaseBatchSize()
    }

    if pm.metrics.ErrorRate > 5.0 {
        // å¯ç”¨æ›´ä¸¥æ ¼çš„é‡‡æ ·
        enableStrictSampling()
    }

    if pm.metrics.MemoryUsage > 500*1024*1024 { // 500MB
        // å¢åŠ åƒåœ¾å›æ”¶
        runtime.GC()
    }
}
```

### 2. è‡ªåŠ¨è°ƒä¼˜

```go
// è‡ªåŠ¨è°ƒä¼˜å™¨
type AutoOptimizer struct {
    monitor   *PerformanceMonitor
    settings *OptimizationSettings
    lastAdjust time.Time
    history   []PerformanceMetrics
    maxHistory int
}

type OptimizationSettings struct {
    BatchSize      int
    SampleRate      float64
    LogLevel       logger.Level
    Compression    bool
    ConnectionPool  int
    FlushInterval  time.Duration
}

func NewAutoOptimizer(monitor *PerformanceMonitor) *AutoOptimizer {
    return &AutoOptimizer{
        monitor:   monitor,
        settings: &OptimizationSettings{
            BatchSize:     100,
            SampleRate:     1.0,
            LogLevel:       logger.InfoLevel,
            Compression:   true,
            ConnectionPool: 10,
            FlushInterval: 1 * time.Second,
        },
        history:       make([]PerformanceMetrics, 0, 100),
        maxHistory:    100,
    }
}

func (ao *AutoOptimizer) Optimize() {
    metrics := ao.monitor.metrics
    ao.history = append(ao.history, *metrics)

    if len(ao.history) > ao.maxHistory {
        ao.history = ao.history[1:]
    }

    // å¦‚æœè·ç¦»ä¸Šæ¬¡è°ƒæ•´æ—¶é—´è¶…è¿‡30ç§’ï¼Œè¿›è¡Œä¼˜åŒ–
    if time.Since(ao.lastAdjust) < 30*time.Second {
        return
    }

    // åˆ†æè¶‹åŠ¿
    trends := ao.analyzeTrends()

    // æ ¹æ®è¶‹åŠ¿è°ƒæ•´è®¾ç½®
    ao.adjustSettings(trends)

    ao.lastAdjust = time.Now()
}

func (ao *AutoOptimizer) analyzeTrends() *TrendAnalysis {
    if len(ao.history) < 2 {
        return &TrendAnalysis{}
    }

    recent := ao.history[len(ao.history)-1]
    previous := ao.history[len(ao.history)-2]

    return &TrendAnalysis{
        LatencyTrend:    calculateTrend(float64(previous.Latency.Nanoseconds()), float64(recent.Latency.Nanoseconds())),
        ThroughputTrend: calculateTrend(previous.Throughput, recent.Throughput),
        MemoryTrend:    calculateTrend(float64(previous.Memory), float64(recent.Memory)),
        ErrorRateTrend: calculateTrend(previous.ErrorRate, recent.ErrorRate),
    }
}

type TrendAnalysis struct {
    LatencyTrend     float64  // 1.0 è¡¨ç¤ºç¨³å®šï¼Œ>1.0 è¡¨ç¤ºå¢é•¿
    ThroughputTrend   float64
    MemoryTrend     float64
    ErrorRateTrend   float64
}

func calculateTrend(old, new float64) float64 {
    if old == 0 {
        return 1.0
    }
    return new / old
}

func (ao *AutoOptimizer) adjustSettings(trends *TrendAnalysis) {
    settings := ao.settings

    // æ ¹æ®å»¶è¿Ÿè¶‹åŠ¿è°ƒæ•´æ‰¹é‡å¤§å°
    if trends.LatencyTrend > 1.2 { // å»¶è¿Ÿå¢åŠ  20%
        newBatchSize := int(float64(settings.BatchSize) * 1.2)
        if newBatchSize > 1000 {
            newBatchSize = 1000
        }
        settings.BatchSize = newBatchSize
    } else if trends.LatencyTrend < 0.8 && settings.BatchSize > 50 { // å»¶è¿Ÿå‡å°‘ 20%
        newBatchSize := int(float64(settings.BatchSize) * 0.8)
        settings.BatchSize = newBatchSize
    }

    // æ ¹æ®é”™è¯¯ç‡è°ƒæ•´é‡‡æ ·ç‡
    if trends.ErrorRateTrend > 1.5 { // é”™è¯¯ç‡å¢åŠ  50%
        newSampleRate := settings.SampleRate * 0.8
        if newSampleRate < 0.1 {
            newSampleRate = 0.1
        }
        settings.SampleRate = newSampleRate
    } else if trends.ErrorRateTrend < 0.5 && settings.SampleRate < 1.0 { // é”™è¯¯ç‡é™ä½
        newSampleRate := settings.SampleRate * 1.2
        if newSampleRate > 1.0 {
            newSampleRate = 1.0
        }
        settings.SampleRate = newSampleRate
    }

    // æ ¹æ®å†…å­˜ä½¿ç”¨è°ƒæ•´è¿æ¥æ± å¤§å°
    if trends.MemoryTrend > 1.3 { // å†…å­˜ä½¿ç”¨å¢åŠ  30%
        newPoolSize := int(float64(settings.ConnectionPool) * 1.2)
        if newPoolSize > 50 {
            newPoolSize = 50
        }
        settings.ConnectionPool = newPoolSize
    }

    logger.Info("è‡ªåŠ¨è°ƒä¼˜é…ç½®",
        logger.Int("batch_size", settings.BatchSize),
        logger.Float64("sample_rate", settings.SampleRate),
        logger.String("log_level", settings.Level.String()),
        logger.Bool("compression", settings.Compression),
        logger.Int("connection_pool", settings.ConnectionPool))
}
```

è¿™ä¸ªæ€§èƒ½ä¼˜åŒ–æŒ‡å—æ¶µç›–äº† EchoMind æ—¥å¿—æ¡†æ¶çš„æ‰€æœ‰å…³é”®æ€§èƒ½ä¼˜åŒ–ç‚¹ï¼ŒåŒ…æ‹¬æ‰¹é‡å¤„ç†ã€å†…å­˜ç®¡ç†ã€ç½‘ç»œä¼ è¾“ã€å­˜å‚¨ä¼˜åŒ–ã€å¹¶å‘å¤„ç†ç­‰æ–¹é¢ï¼Œä»¥åŠè‡ªåŠ¨åŒ–çš„ç›‘æ§å’Œè°ƒä¼˜æœºåˆ¶ã€‚é€šè¿‡å®æ–½è¿™äº›ä¼˜åŒ–ç­–ç•¥ï¼Œå¯ä»¥å°†æ—¥å¿—æ¡†æ¶çš„æ€§èƒ½æå‡ 5-10 å€ã€‚